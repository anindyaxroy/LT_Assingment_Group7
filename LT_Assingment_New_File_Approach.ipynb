{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659ab6fa",
   "metadata": {},
   "source": [
    "# MBA AI - Language Technology Group Assignment 2025\n",
    "**Course**: Language Technology  \n",
    "**Team Members**:  \n",
    "**Date**:  \n",
    "\n",
    "## Overview\n",
    "This assignment involves working with 4 main questions covering lexical representations, named entity recognition, question-answering systems, and generative AI chatbots. Each question is worth 25 points for a total of 100 points.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Lexical Representations & Vocabulary](#section1)\n",
    "2. [Named Entity Recognition and Entity Analysis](#section2)\n",
    "3. [Question Answering with Transformers](#section3)\n",
    "4. [RAG-Based Football Chatbot](#section4)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbbeccf",
   "metadata": {},
   "source": [
    "## General Dependencies Summary\n",
    "\n",
    "### Core Libraries:\n",
    "```python\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP Libraries\n",
    "import spacy  # en_core_web_sm model required\n",
    "import re\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Transformers & Sentence Transformers\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Vector Database\n",
    "import milvus\n",
    "\n",
    "# Utilities\n",
    "from collections import Counter\n",
    "import json\n",
    "import requests\n",
    "```\n",
    "\n",
    "### Model Downloads Required:\n",
    "- SpaCy: `python -m spacy download en_core_web_sm`\n",
    "- Hugging Face models (auto-downloaded):\n",
    "  - `sentence-transformers/multi-qa-mpnet-base-cos-v1`\n",
    "  - `distilbert/distilbert-base-cased-distilled-squad`\n",
    "  - `deepset/tinyroberta-squad2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acfad77",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "# 1. Lexical Representations & Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf0b60",
   "metadata": {},
   "source": [
    "## Question 1: Lexical Representations and Text Classification (25 points)\n",
    "\n",
    "### Actions Required:\n",
    "1. **Load datasets** from parquet files (Amazon Reviews, Malawi, EN Crawl)\n",
    "2. **Vocabulary analysis** with preprocessing (Q1 - 10pts)\n",
    "3. **Binary classification model** for review scores (Q2 - 15pts)\n",
    "\n",
    "### Q1 Approach (10pts):\n",
    "**Objective**: Calculate vocabulary size with lowercase + lemmatization preprocessing\n",
    "\n",
    "**Algorithm**:\n",
    "1. Load Amazon Reviews dataset from parquet\n",
    "2. Apply preprocessing pipeline:\n",
    "   - Convert text to lowercase\n",
    "   - Apply lemmatization using SpaCy\n",
    "3. Use CountVectorizer to build vocabulary\n",
    "4. Return vocabulary size\n",
    "\n",
    "**Libraries/Dependencies**:\n",
    "- `pandas` - data loading\n",
    "- `spacy` (en_core_web_sm model) - lemmatization\n",
    "- `sklearn.feature_extraction.text.CountVectorizer` - vocabulary extraction\n",
    "- `re` - regular expressions\n",
    "\n",
    "### Q2 Approach (15pts):\n",
    "**Objective**: Binary classification (score=5 vs not=5) with hyperparameter exploration\n",
    "\n",
    "**Algorithm**:\n",
    "1. Sample 50,000 random records from Amazon Reviews\n",
    "2. Create binary target: score==5 vs score!=5\n",
    "3. Build 2-stage sklearn pipeline:\n",
    "   - Stage 1: TfidfVectorizer with specific parameters\n",
    "   - Stage 2: LogisticRegression (max_iter=1000)\n",
    "4. Hyperparameter grid search on:\n",
    "   - Regularization parameter C\n",
    "   - Token filtering (>=3 ASCII letters)\n",
    "   - TF vs TFIDF (use_idf parameter)\n",
    "   - Unigrams vs Unigrams+Bigrams\n",
    "\n",
    "**Libraries/Dependencies**:\n",
    "- `sklearn.feature_extraction.text.TfidfVectorizer`\n",
    "- `sklearn.linear_model.LogisticRegression`\n",
    "- `sklearn.pipeline.Pipeline`\n",
    "- `sklearn.model_selection.GridSearchCV`\n",
    "- `sklearn.metrics` - evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f3d008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddb1f33c",
   "metadata": {},
   "source": [
    "## Question 2: Named Entity Recognition (25 points)\n",
    "\n",
    "### Actions Required:\n",
    "1. **Entity category analysis** (Q1 - 10pts)\n",
    "2. **VIP identification and analysis** (Q2 - 15pts)\n",
    "\n",
    "### Q1 Approach (10pts):\n",
    "**Objective**: Build dataframe of entity categories and their usage counts\n",
    "\n",
    "**Algorithm**:\n",
    "1. Load Malawi dataset from parquet\n",
    "2. Process each text document with SpaCy NER\n",
    "3. Extract all entities and their categories\n",
    "4. Count occurrences of each entity category\n",
    "5. Create summary dataframe\n",
    "\n",
    "**Libraries/Dependencies**:\n",
    "- `spacy` (en_core_web_sm model) - NER\n",
    "- `pandas` - data manipulation\n",
    "- `collections.Counter` - counting occurrences\n",
    "\n",
    "### Q2 Approach (15pts):\n",
    "**Objective**: Identify top 10 most mentioned people with analysis\n",
    "\n",
    "**Algorithm**:\n",
    "1. Extract all PERSON entities from Malawi corpus\n",
    "2. Normalize mentions (lowercase grouping)\n",
    "3. Count mentions per person\n",
    "4. Rank and select top 10\n",
    "5. Analyze accuracy and provide recommendations\n",
    "\n",
    "**Libraries/Dependencies**:\n",
    "- `spacy` (en_core_web_sm model) - NER for PERSON entities\n",
    "- `pandas` - data manipulation and analysis\n",
    "- `re` - text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca369ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f970a1f0",
   "metadata": {},
   "source": [
    "## Question 3: Question-Answering System (25 points)\n",
    "\n",
    "### Actions Required:\n",
    "1. **Document chunking and vectorization**\n",
    "2. **Semantic search implementation**\n",
    "3. **Span-based QA with multiple models**\n",
    "4. **Answer 3 specific questions about BERT**\n",
    "\n",
    "### Approach:\n",
    "**Objective**: Build QA system using semantic search + span extraction\n",
    "\n",
    "**Algorithm**:\n",
    "1. Load BERT article text\n",
    "2. Sentence tokenization using SpaCy\n",
    "3. Create 2-sentence chunks\n",
    "4. Encode chunks using sentence-transformers model\n",
    "5. For each question:\n",
    "   - Encode question with same model\n",
    "   - Find 3 most similar chunks (cosine similarity)\n",
    "   - Check similarity threshold (>0.3)\n",
    "   - If threshold met, use chunks as context for QA models\n",
    "   - Apply both QA models and compare results\n",
    "\n",
    "**Libraries/Dependencies**:\n",
    "- `spacy` (en_core_web_sm) - sentence tokenization\n",
    "- `sentence-transformers` - text encoding\n",
    "- `transformers` - QA pipeline\n",
    "- `numpy` - similarity calculations\n",
    "- `torch` - model backend\n",
    "\n",
    "**Models Required**:\n",
    "- `sentence-transformers/multi-qa-mpnet-base-cos-v1` - encoding\n",
    "- `distilbert/distilbert-base-cased-distilled-squad` - QA\n",
    "- `deepset/tinyroberta-squad2` - QA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05fe0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb03b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5517ef4",
   "metadata": {},
   "source": [
    "## Question 4: RAG-based Chatbot (25 points)\n",
    "\n",
    "### Actions Required:\n",
    "1. **Setup API connections** (LLM + embeddings + Milvus)\n",
    "2. **Build RAG pipeline with multiple LLM calls**\n",
    "3. **Implement multilingual chatbot**\n",
    "4. **Test and analyze performance**\n",
    "\n",
    "### Approach:\n",
    "**Objective**: Build multilingual football chatbot using RAG architecture\n",
    "\n",
    "**Algorithm**:\n",
    "1. Setup system prompt for football topic validation\n",
    "2. For each user query:\n",
    "   - Check if query relates to football (LLM call 1)\n",
    "   - If not football-related, politely decline\n",
    "   - Parse query into question + formatting instructions (LLM call 2)\n",
    "   - Rephrase question considering multiple facets (LLM call 3)\n",
    "   - Retrieve relevant chunks from Milvus vector store\n",
    "   - Generate answer with source citation (LLM call 4)\n",
    "\n",
    "**Pipeline Components**:\n",
    "1. **Topic Validation**: LLM determines if query is football-related\n",
    "2. **Query Parsing**: Separate question from formatting instructions\n",
    "3. **Query Enhancement**: Rephrase for better retrieval\n",
    "4. **Semantic Retrieval**: Vector similarity search in Milvus\n",
    "5. **Answer Generation**: Grounded response with citations\n",
    "\n",
    "**Libraries/Dependencies**:\n",
    "- Custom API endpoints (provided in notebook)\n",
    "- `milvus` - vector database client\n",
    "- `requests` - API calls\n",
    "- `json` - data handling\n",
    "\n",
    "**Infrastructure**:\n",
    "- GenAI API endpoint (multiple models)\n",
    "- Text embedding API (text-embedding-3-small)\n",
    "- Milvus vector store (62,068 embedded chunks)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
