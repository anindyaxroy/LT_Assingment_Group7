{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: RAG-based Chatbot (25 points)\n",
        "\n",
        "\n",
        "\n",
        "### Actions Required:\n",
        "1. **Setup API connections** (LLM + embeddings + Milvus)\n",
        "2. **Build RAG pipeline with multiple LLM calls**\n",
        "3. **Implement multilingual chatbot**\n",
        "4. **Test and analyze performance**\n",
        "\n",
        "### Approach:\n",
        "**Objective**: Build multilingual football chatbot using RAG architecture\n",
        "\n",
        "**Algorithm**:\n",
        "1. Setup system prompt for football topic validation\n",
        "2. For each user query:\n",
        "   - Check if query relates to football (LLM call 1)\n",
        "   - If not football-related, politely decline\n",
        "   - Parse query into question + formatting instructions (LLM call 2)\n",
        "   - Rephrase question considering multiple facets (LLM call 3)\n",
        "   - Retrieve relevant chunks from Milvus vector store\n",
        "   - Generate answer with source citation (LLM call 4)\n",
        "\n",
        "**Pipeline Components**:\n",
        "1. **Topic Validation**: LLM determines if query is football-related\n",
        "2. **Query Parsing**: Separate question from formatting instructions\n",
        "3. **Query Enhancement**: Rephrase for better retrieval\n",
        "4. **Semantic Retrieval**: Vector similarity search in Milvus\n",
        "5. **Answer Generation**: Grounded response with citations\n",
        "\n",
        "**Libraries/Dependencies**:\n",
        "- Custom API endpoints (provided in notebook)\n",
        "- `milvus` - vector database client\n",
        "- `requests` - API calls\n",
        "- `json` - data handling\n",
        "\n",
        "**Infrastructure**:\n",
        "- GenAI API endpoint (multiple models)\n",
        "- Text embedding API (text-embedding-3-small)\n",
        "- Milvus vector store (62,068 embedded chunks)"
      ],
      "metadata": {
        "id": "ocEe6-MFaFhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access to a multiple-LLM endpoint This endpoint implements the API model of  AzureAI and usable with the classes AzureAIChatCompletionsModel and AzureAIEmbeddingsModel from langchain\n",
        "You have to install the following packages in your python environment:\n",
        "  \n",
        "*   langchain\n",
        "*   langchain-core\n",
        "*   langchain-azure-ai\n",
        "*   langchain-milvus"
      ],
      "metadata": {
        "id": "xv-uKzxpay1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary LangChain packages for AzureAI endpoint + Milvus support\n",
        "!pip install langchain langchain-core langchain-azure-ai langchain-milvus langchain-community\n",
        "# Import essential classes for AzureAI interaction\n",
        "from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n",
        "from langchain_azure_ai.embeddings import AzureAIEmbeddingsModel\n",
        "from langchain_community.vectorstores import Milvus\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.tools import Tool\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUHeEdv6cHWJ",
        "outputId": "08bad60e-9a55-43bd-9b90-feecc3459f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.67)\n",
            "Collecting langchain-azure-ai\n",
            "  Downloading langchain_azure_ai-0.1.4-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langchain-milvus\n",
            "  Downloading langchain_milvus-0.2.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.14.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from langchain-azure-ai) (3.11.15)\n",
            "Collecting azure-ai-inference<2.0.0,>=1.0.0b7 (from azure-ai-inference[opentelemetry]<2.0.0,>=1.0.0b7->langchain-azure-ai)\n",
            "  Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting azure-core<2.0.0,>=1.32.0 (from langchain-azure-ai)\n",
            "  Downloading azure_core-1.35.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-cosmos<5.0.0,>=4.9.0 (from langchain-azure-ai)\n",
            "  Downloading azure_cosmos-4.9.0-py3-none-any.whl.metadata (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-identity<2.0.0,>=1.15.0 (from langchain-azure-ai)\n",
            "  Downloading azure_identity-1.23.0-py3-none-any.whl.metadata (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-openai<0.4.0,>=0.3.0 (from langchain-azure-ai)\n",
            "  Downloading langchain_openai-0.3.27-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-azure-ai) (2.0.2)\n",
            "Collecting pymilvus<3.0,>=2.5.7 (from langchain-milvus)\n",
            "  Downloading pymilvus-2.5.12-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.10.0->langchain-azure-ai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.10.0->langchain-azure-ai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.10.0->langchain-azure-ai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.10.0->langchain-azure-ai) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.10.0->langchain-azure-ai) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.10.0->langchain-azure-ai) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.10.0->langchain-azure-ai) (1.20.1)\n",
            "Collecting isodate>=0.6.1 (from azure-ai-inference<2.0.0,>=1.0.0b7->azure-ai-inference[opentelemetry]<2.0.0,>=1.0.0b7->langchain-azure-ai)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting azure-core-tracing-opentelemetry (from azure-ai-inference[opentelemetry]<2.0.0,>=1.0.0b7->langchain-azure-ai)\n",
            "  Downloading azure_core_tracing_opentelemetry-1.0.0b12-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core<2.0.0,>=1.32.0->langchain-azure-ai) (1.17.0)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.11/dist-packages (from azure-identity<2.0.0,>=1.15.0->langchain-azure-ai) (43.0.3)\n",
            "Collecting msal>=1.30.0 (from azure-identity<2.0.0,>=1.15.0->langchain-azure-ai)\n",
            "  Downloading msal-1.32.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msal-extensions>=1.2.0 (from azure-identity<2.0.0,>=1.15.0->langchain-azure-ai)\n",
            "  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai<0.4.0,>=0.3.0->langchain-azure-ai) (1.93.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai<0.4.0,>=0.3.0->langchain-azure-ai) (0.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: setuptools>69 in /usr/local/lib/python3.11/dist-packages (from pymilvus<3.0,>=2.5.7->langchain-milvus) (75.2.0)\n",
            "Collecting grpcio<=1.67.1,>=1.49.1 (from pymilvus<3.0,>=2.5.7->langchain-milvus)\n",
            "  Downloading grpcio-1.67.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus<3.0,>=2.5.7->langchain-milvus) (5.29.5)\n",
            "Collecting ujson>=2.0.0 (from pymilvus<3.0,>=2.5.7->langchain-milvus)\n",
            "  Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.11/dist-packages (from pymilvus<3.0,>=2.5.7->langchain-milvus) (2.2.2)\n",
            "Collecting milvus-lite>=2.4.0 (from pymilvus<3.0,>=2.5.7->langchain-milvus)\n",
            "  Downloading milvus_lite-2.5.1-py3-none-manylinux2014_x86_64.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->langchain-azure-ai) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from milvus-lite>=2.4.0->pymilvus<3.0,>=2.5.7->langchain-milvus) (4.67.1)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity<2.0.0,>=1.15.0->langchain-azure-ai) (2.10.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai<0.4.0,>=0.3.0->langchain-azure-ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai<0.4.0,>=0.3.0->langchain-azure-ai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai<0.4.0,>=0.3.0->langchain-azure-ai) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus<3.0,>=2.5.7->langchain-milvus) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus<3.0,>=2.5.7->langchain-milvus) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus<3.0,>=2.5.7->langchain-milvus) (2025.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai<0.4.0,>=0.3.0->langchain-azure-ai) (2024.11.6)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting opentelemetry-api>=1.12.0 (from azure-core-tracing-opentelemetry->azure-ai-inference[opentelemetry]<2.0.0,>=1.0.0b7->langchain-azure-ai)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->langchain-azure-ai) (2.22)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.12.0->azure-core-tracing-opentelemetry->azure-ai-inference[opentelemetry]<2.0.0,>=1.0.0b7->langchain-azure-ai) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.12.0->azure-core-tracing-opentelemetry->azure-ai-inference[opentelemetry]<2.0.0,>=1.0.0b7->langchain-azure-ai) (3.23.0)\n",
            "Downloading langchain_azure_ai-0.1.4-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_milvus-0.2.1-py3-none-any.whl (36 kB)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_ai_inference-1.0.0b9-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.35.0-py3-none-any.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_cosmos-4.9.0-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.2/303.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_identity-1.23.0-py3-none-any.whl (186 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.1/186.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading langchain_openai-0.3.27-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymilvus-2.5.12-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.4/231.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.67.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading milvus_lite-2.5.1-py3-none-manylinux2014_x86_64.whl (55.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal-1.32.3-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.4/115.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core_tracing_opentelemetry-1.0.0b12-py3-none-any.whl (11 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ujson, python-dotenv, mypy-extensions, milvus-lite, marshmallow, isodate, httpx-sse, grpcio, typing-inspect, opentelemetry-api, azure-core, pymilvus, pydantic-settings, dataclasses-json, azure-cosmos, azure-core-tracing-opentelemetry, azure-ai-inference, msal, msal-extensions, langchain-openai, langchain-milvus, azure-identity, langchain-community, langchain-azure-ai\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.73.1\n",
            "    Uninstalling grpcio-1.73.1:\n",
            "      Successfully uninstalled grpcio-1.73.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.2 requires grpcio>=1.71.2, but you have grpcio 1.67.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed azure-ai-inference-1.0.0b9 azure-core-1.35.0 azure-core-tracing-opentelemetry-1.0.0b12 azure-cosmos-4.9.0 azure-identity-1.23.0 dataclasses-json-0.6.7 grpcio-1.67.1 httpx-sse-0.4.1 isodate-0.7.2 langchain-azure-ai-0.1.4 langchain-community-0.3.27 langchain-milvus-0.2.1 langchain-openai-0.3.27 marshmallow-3.26.1 milvus-lite-2.5.1 msal-1.32.3 msal-extensions-1.3.1 mypy-extensions-1.1.0 opentelemetry-api-1.34.1 pydantic-settings-2.10.1 pymilvus-2.5.12 python-dotenv-1.1.1 typing-inspect-0.9.0 ujson-5.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### API Key Congiguration for LLM Model : :\n",
        "\n",
        "```\n",
        "# in2LIRnDlbyb6FRW\n",
        "```\n",
        "\n",
        "Please do not change!"
      ],
      "metadata": {
        "id": "w3knjo3sbtNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LLM_API_ENDPOINT = \"http://188.166.132.29:8080/models\"\n",
        "LLM_API_KEY = \"in2LIRnDlbyb6FRW\"\n",
        "\n",
        "MILVUS_ENDPOINT = \"http://188.166.132.29:19530\"\n",
        "\n",
        "MILVUS_DB_NAME = \"cahiers_du_foot\"\n",
        "MILVUS_COLLECTION_NAME = \"articles\"\n",
        "\n",
        "MILVUS_TOKEN_STUDENT = \"student:2mZkrPRZXOKsJfyRscYyoL0M7UyL6y\""
      ],
      "metadata": {
        "id": "s0EdKMAFaJ5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "model = AzureAIChatCompletionsModel(\n",
        "    endpoint=LLM_API_ENDPOINT,\n",
        "    credential=LLM_API_KEY,\n",
        "    model=\"gpt-4o\",     # model_name in [\"Phi-4\", \"gpt-4o\", \"DeepSeek-R1\", \"Mistral-Nemo\"]\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Translate the following from English into French\"),\n",
        "    HumanMessage(content=\"Hi there, what are we going to cook today ?\"),\n",
        "]\n",
        "\n",
        "response = model.invoke(messages)\n",
        "print(response.content)   # something like \"Bonjour, qu'est-ce que nous allons cuisiner aujourd'hui ?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ8-Vb-ocETJ",
        "outputId": "04eb2774-3c27-48e0-b8e6-1113cb6d7ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Salut, qu'allons-nous cuisiner aujourd'hui ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LLM model\n",
        "model = AzureAIChatCompletionsModel(\n",
        "    endpoint=LLM_API_ENDPOINT,\n",
        "    credential=LLM_API_KEY,\n",
        "    model=\"gpt-4o\",  # Available models: [\"Phi-4\", \"gpt-4o\", \"DeepSeek-R1\", \"Mistral-Nemo\"]\n",
        ")\n",
        "\n",
        "# Initialize embeddings model\n",
        "embeddings = AzureAIEmbeddingsModel(\n",
        "    endpoint=LLM_API_ENDPOINT,\n",
        "    credential=LLM_API_KEY,\n",
        "    model=\"text-embedding-3-small\"\n",
        ")"
      ],
      "metadata": {
        "id": "lLZ6bUR3-yyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Milvus vector store\n",
        "vectorstore = Milvus(\n",
        "    embedding_function=embeddings,\n",
        "    connection_args={\"uri\": MILVUS_ENDPOINT, \"token\": MILVUS_TOKEN_STUDENT},\n",
        "    collection_name=MILVUS_COLLECTION_NAME,\n",
        "    # database_name=MILVUS_DB_NAME # Removed database_name argument\n",
        ")"
      ],
      "metadata": {
        "id": "xf-s_5uO-7Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Pipeline Components"
      ],
      "metadata": {
        "id": "oixisDwwo1mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List # Import Dict and List for type hinting\n",
        "from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.documents import Document # Import Document for type hinting\n",
        "import json # Import json for parsing\n",
        "\n",
        "\n",
        "class FootballChatbot:\n",
        "    def __init__(self, model, vectorstore):\n",
        "        self.model = model\n",
        "        self.vectorstore = vectorstore\n",
        "        self.setup_prompts()\n",
        "\n",
        "    def setup_prompts(self):\n",
        "        \"\"\"Setup all the prompt templates for different pipeline stages\"\"\"\n",
        "\n",
        "        # Step 1: Topic Validation Prompt\n",
        "        self.topic_validation_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=\"\"\"You are a helpful assistant that determines if a user query is related to football (soccer).\n",
        "            Consider queries about players, teams, matches, tournaments, tactics, history, rules, statistics, and football culture as football-related.\n",
        "            Respond with only 'yes' if the query is football-related, or 'no' if it's not. No other text.\"\"\"),\n",
        "            HumanMessage(content=\"{query}\")\n",
        "        ])\n",
        "\n",
        "        # Step 2: Query Parsing Prompt\n",
        "        self.query_parsing_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=\"\"\"You are an expert at parsing user queries into two components:\n",
        "            1. The core question about football\n",
        "            2. Any specific formatting instructions (e.g., \"give me 4 bullet points\", \"write in French\", \"brief summary\")\n",
        "\n",
        "            Return your response in JSON format:\n",
        "            {\n",
        "                \"question\": \"the core football question\",\n",
        "                \"formatting_instructions\": \"any specific formatting requests or 'none' if no special formatting requested\"\n",
        "            }\"\"\"),\n",
        "            HumanMessage(content=\"{query}\")\n",
        "        ])\n",
        "\n",
        "        # Step 3: Query Enhancement Prompt\n",
        "        self.query_enhancement_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=\"\"\"You are an expert at rephrasing football queries to improve information retrieval.\n",
        "            Consider multiple facets of the question and rephrase it to capture different aspects that might be relevant.\n",
        "            Generate 2-3 alternative phrasings that would help find comprehensive information.\n",
        "\n",
        "            Return your response as a JSON list:\n",
        "            [\"original question\", \"alternative phrasing 1\", \"alternative phrasing 2\"]\"\"\"),\n",
        "            HumanMessage(content=\"{question}\")\n",
        "        ])\n",
        "\n",
        "        # Step 4: Answer Generation Prompt\n",
        "        self.answer_generation_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=\"\"\"You are an expert football analyst who provides comprehensive answers based on retrieved information.\n",
        "\n",
        "            Guidelines:\n",
        "            1. Use ONLY the provided context to answer the question\n",
        "            2. Always cite your sources by mentioning the article titles\n",
        "            3. If the context doesn't contain enough information, acknowledge this\n",
        "            4. Maintain accuracy and avoid speculation\n",
        "            5. Follow any specific formatting instructions provided\n",
        "            6. Respond in the same language as the user's question unless otherwise specified\n",
        "\n",
        "            Context: {context}\n",
        "\n",
        "            Question: {question}\n",
        "            Formatting Instructions: {formatting_instructions}\n",
        "\n",
        "            Provide a well-structured answer with proper source citations.\"\"\"),\n",
        "            HumanMessage(content=\"Please provide a comprehensive answer based on the context provided.\")\n",
        "        ])\n",
        "\n",
        "    def validate_topic(self, query: str) -> bool:\n",
        "        \"\"\"Step 1: Check if query is football-related\"\"\"\n",
        "        chain = self.topic_validation_prompt | self.model\n",
        "        response = chain.invoke({\"query\": query})\n",
        "        return response.content.strip().lower() == \"yes\"\n",
        "\n",
        "    def parse_query(self, query: str) -> Dict[str, str]:\n",
        "        \"\"\"Step 2: Parse query into question and formatting instructions\"\"\"\n",
        "        chain = self.query_parsing_prompt | self.model\n",
        "        response = chain.invoke({\"query\": query})\n",
        "        try:\n",
        "            return json.loads(response.content)\n",
        "        except json.JSONDecodeError:\n",
        "            return {\"question\": query, \"formatting_instructions\": \"none\"}\n",
        "\n",
        "    def enhance_query(self, question: str) -> List[str]:\n",
        "        \"\"\"Step 3: Generate multiple query variants for better retrieval\"\"\"\n",
        "        chain = self.query_enhancement_prompt | self.model\n",
        "        response = chain.invoke({\"question\": question})\n",
        "        try:\n",
        "            return json.loads(response.content)\n",
        "        except json.JSONDecodeError:\n",
        "            return [question]\n",
        "\n",
        "    def retrieve_relevant_chunks(self, queries: List[str], k: int = 5) -> List[Document]:\n",
        "        \"\"\"Step 4: Retrieve relevant chunks from Milvus\"\"\"\n",
        "        all_docs = []\n",
        "        for query in queries:\n",
        "            docs = self.vectorstore.similarity_search(query, k=k)\n",
        "            all_docs.extend(docs)\n",
        "\n",
        "        # Remove duplicates based on content\n",
        "        unique_docs = []\n",
        "        seen_content = set()\n",
        "        for doc in all_docs:\n",
        "            if doc.page_content not in seen_content:\n",
        "                unique_docs.append(doc)\n",
        "                seen_content.add(doc.page_content)\n",
        "\n",
        "        return unique_docs[:k*2]  # Return top 2k results\n",
        "\n",
        "    def generate_answer(self, question: str, formatting_instructions: str, context_docs: List[Document]) -> str:\n",
        "        \"\"\"Step 5: Generate final answer with citations\"\"\"\n",
        "        # Prepare context with source information\n",
        "        context_with_sources = []\n",
        "        for doc in context_docs:\n",
        "            metadata = doc.metadata\n",
        "            title = metadata.get('title', 'Unknown Article')\n",
        "            url = metadata.get('url', 'URL not available')\n",
        "            context_with_sources.append(f\"Article: {title}\\nURL: {url}\\nContent: {doc.page_content}\\n\")\n",
        "\n",
        "        context = \"\\n---\\n\".join(context_with_sources)\n",
        "\n",
        "        chain = self.answer_generation_prompt | self.model\n",
        "        response = chain.invoke({\n",
        "            \"context\": context,\n",
        "            \"question\": question,\n",
        "            \"formatting_instructions\": formatting_instructions\n",
        "        })\n",
        "\n",
        "        return response.content\n",
        "\n",
        "    def chat(self, user_query: str) -> str:\n",
        "        \"\"\"Main chatbot function that orchestrates the entire pipeline\"\"\"\n",
        "        print(f\"Processing query: {user_query}\")\n",
        "\n",
        "        # Step 1: Topic Validation\n",
        "        if not self.validate_topic(user_query):\n",
        "            return \"Je suis désolé, mais je ne peux répondre qu'aux questions liées au football. Pouvez-vous me poser une question sur le football s'il vous plaît?\"\n",
        "\n",
        "        # Step 2: Query Parsing\n",
        "        parsed_query = self.parse_query(user_query)\n",
        "        question = parsed_query[\"question\"]\n",
        "        formatting_instructions = parsed_query[\"formatting_instructions\"]\n",
        "\n",
        "        print(f\"Parsed question: {question}\")\n",
        "        print(f\"Formatting instructions: {formatting_instructions}\")\n",
        "\n",
        "        # Step 3: Query Enhancement\n",
        "        enhanced_queries = self.enhance_query(question)\n",
        "        print(f\"Enhanced queries: {enhanced_queries}\")\n",
        "\n",
        "        # Step 4: Retrieval\n",
        "        relevant_docs = self.retrieve_relevant_chunks(enhanced_queries)\n",
        "        print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
        "\n",
        "        # Step 5: Answer Generation\n",
        "        answer = self.generate_answer(question, formatting_instructions, relevant_docs)\n",
        "\n",
        "        return answer"
      ],
      "metadata": {
        "id": "BJZ-YAPLo6gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize and Test the Chatbot\n"
      ],
      "metadata": {
        "id": "775621rBpQId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the chatbot\n",
        "chatbot = FootballChatbot(model, vectorstore)\n",
        "\n",
        "# Test the chatbot with various queries\n",
        "test_queries = [\n",
        "    \"Tell me in 4 bullet points what are the issues of video assisted refereeing\",\n",
        "    \"Tell me 3 things I should know about Johan Cruyff (incorrect spelling is intentional)\",\n",
        "    \"What is the capital of France?\",  # Non-football query\n",
        "    \"Raconte-moi l'histoire du football français\",  # French query\n",
        "    \"What are the tactical innovations in modern football?\",\n",
        "    \"Who are the greatest French footballers of all time?\"\n",
        "]\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"FOOTBALL CHATBOT TESTING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for i, query in enumerate(test_queries, 1):\n",
        "    print(f\"\\n{i}. Query: {query}\")\n",
        "    print(\"-\" * 40)\n",
        "    response = chatbot.chat(query)\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMGSjZ7IpGY5",
        "outputId": "e23cd4a8-5c8d-438d-b0b9-e9a4b721c810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "FOOTBALL CHATBOT TESTING\n",
            "==================================================\n",
            "\n",
            "1. Query: Tell me in 4 bullet points what are the issues of video assisted refereeing\n",
            "----------------------------------------\n",
            "Processing query: Tell me in 4 bullet points what are the issues of video assisted refereeing\n",
            "Response: Je suis désolé, mais je ne peux répondre qu'aux questions liées au football. Pouvez-vous me poser une question sur le football s'il vous plaît?\n",
            "==================================================\n",
            "\n",
            "2. Query: Tell me 3 things I should know about Johan Cruyff (incorrect spelling is intentional)\n",
            "----------------------------------------\n",
            "Processing query: Tell me 3 things I should know about Johan Cruyff (incorrect spelling is intentional)\n",
            "Response: Je suis désolé, mais je ne peux répondre qu'aux questions liées au football. Pouvez-vous me poser une question sur le football s'il vous plaît?\n",
            "==================================================\n",
            "\n",
            "3. Query: What is the capital of France?\n",
            "----------------------------------------\n",
            "Processing query: What is the capital of France?\n",
            "Response: Je suis désolé, mais je ne peux répondre qu'aux questions liées au football. Pouvez-vous me poser une question sur le football s'il vous plaît?\n",
            "==================================================\n",
            "\n",
            "4. Query: Raconte-moi l'histoire du football français\n",
            "----------------------------------------\n",
            "Processing query: Raconte-moi l'histoire du football français\n",
            "Response: Je suis désolé, mais je ne peux répondre qu'aux questions liées au football. Pouvez-vous me poser une question sur le football s'il vous plaît?\n",
            "==================================================\n",
            "\n",
            "5. Query: What are the tactical innovations in modern football?\n",
            "----------------------------------------\n",
            "Processing query: What are the tactical innovations in modern football?\n",
            "Response: Je suis désolé, mais je ne peux répondre qu'aux questions liées au football. Pouvez-vous me poser une question sur le football s'il vous plaît?\n",
            "==================================================\n",
            "\n",
            "6. Query: Who are the greatest French footballers of all time?\n",
            "----------------------------------------\n",
            "Processing query: Who are the greatest French footballers of all time?\n",
            "Response: Je suis désolé, mais je ne peux répondre qu'aux questions liées au football. Pouvez-vous me poser une question sur le football s'il vous plaît?\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Analysis Functions\n"
      ],
      "metadata": {
        "id": "Vj85gPnJpcx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Any # Import Any for type hinting\n",
        "\n",
        "def analyze_chatbot_performance(chatbot, test_queries: List[str]) -> Dict[str, Any]:\n",
        "    \"\"\"Analyze the performance of the chatbot across different aspects\"\"\"\n",
        "\n",
        "    results = {\n",
        "        \"total_queries\": len(test_queries),\n",
        "        \"successful_responses\": 0,\n",
        "        \"topic_validation_accuracy\": 0,\n",
        "        \"language_detection\": {\"french\": 0, \"english\": 0, \"other\": 0},\n",
        "        \"response_lengths\": [],\n",
        "        \"processing_times\": []\n",
        "    }\n",
        "\n",
        "    import time\n",
        "\n",
        "    for query in test_queries:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Test topic validation\n",
        "        is_football = chatbot.validate_topic(query)\n",
        "\n",
        "        # Generate response\n",
        "        response = chatbot.chat(query)\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "        results[\"processing_times\"].append(processing_time)\n",
        "\n",
        "        # Analyze response\n",
        "        if response and len(response) > 10:  # Basic success criteria\n",
        "            results[\"successful_responses\"] += 1\n",
        "\n",
        "        results[\"response_lengths\"].append(len(response))\n",
        "\n",
        "        # Simple language detection\n",
        "        if any(word in query.lower() for word in ['raconte', 'français', 'que', 'est', 'le', 'la', 'les']):\n",
        "            results[\"language_detection\"][\"french\"] += 1\n",
        "        elif any(word in query.lower() for word in ['what', 'tell', 'who', 'how', 'when', 'where']):\n",
        "            results[\"language_detection\"][\"english\"] += 1\n",
        "        else:\n",
        "            results[\"language_detection\"][\"other\"] += 1\n",
        "\n",
        "    # Calculate averages\n",
        "    results[\"avg_response_length\"] = sum(results[\"response_lengths\"]) / len(results[\"response_lengths\"])\n",
        "    results[\"avg_processing_time\"] = sum(results[\"processing_times\"]) / len(results[\"processing_times\"])\n",
        "    results[\"success_rate\"] = results[\"successful_responses\"] / results[\"total_queries\"]\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run performance analysis\n",
        "performance_results = analyze_chatbot_performance(chatbot, test_queries)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PERFORMANCE ANALYSIS RESULTS\")\n",
        "print(\"=\"*50)\n",
        "for key, value in performance_results.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_990e7dpiWT",
        "outputId": "58ac780f-18c7-4ae2-e3cd-99471afbeb44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing query: Tell me in 4 bullet points what are the issues of video assisted refereeing\n",
            "Processing query: Tell me 3 things I should know about Johan Cruyff (incorrect spelling is intentional)\n",
            "Processing query: What is the capital of France?\n",
            "Processing query: Raconte-moi l'histoire du football français\n",
            "Processing query: What are the tactical innovations in modern football?\n",
            "Processing query: Who are the greatest French footballers of all time?\n",
            "\n",
            "==================================================\n",
            "PERFORMANCE ANALYSIS RESULTS\n",
            "==================================================\n",
            "total_queries: 6\n",
            "successful_responses: 6\n",
            "topic_validation_accuracy: 0\n",
            "language_detection: {'french': 3, 'english': 3, 'other': 0}\n",
            "response_lengths: [143, 143, 143, 143, 143, 143]\n",
            "processing_times: [1.3128092288970947, 1.2844314575195312, 2.0491738319396973, 1.2805581092834473, 1.329559087753296, 5.036961317062378]\n",
            "avg_response_length: 143.0\n",
            "avg_processing_time: 2.0489155054092407\n",
            "success_rate: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Testing Functions\n"
      ],
      "metadata": {
        "id": "2-e3fSnupoZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multilingual_capabilities(chatbot):\n",
        "    \"\"\"Test the chatbot's ability to handle different languages\"\"\"\n",
        "\n",
        "    multilingual_queries = [\n",
        "        \"What is offside in football?\",  # English\n",
        "        \"Qu'est-ce que le hors-jeu au football?\",  # French\n",
        "        \"Parlez-moi de l'équipe de France\",  # French\n",
        "        \"Tell me about the World Cup history\",  # English\n",
        "        \"Quels sont les plus grands joueurs français?\",  # French\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"MULTILINGUAL TESTING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for query in multilingual_queries:\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        print(\"-\" * 40)\n",
        "        response = chatbot.chat(query)\n",
        "        print(f\"Response: {response[:200]}...\")  # Show first 200 chars\n",
        "        print(\"=\"*50)\n",
        "\n",
        "def test_edge_cases(chatbot):\n",
        "    \"\"\"Test edge cases and error handling\"\"\"\n",
        "\n",
        "    edge_cases = [\n",
        "        \"\",  # Empty query\n",
        "        \"Tell me about basketball\",  # Different sport\n",
        "        \"What is 2+2?\",  # Math question\n",
        "        \"Football\",  # Very short query\n",
        "        \"Tell me everything about football in exactly 1000 words\",  # Complex formatting\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EDGE CASE TESTING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for query in edge_cases:\n",
        "        print(f\"\\nQuery: '{query}'\")\n",
        "        print(\"-\" * 40)\n",
        "        try:\n",
        "            response = chatbot.chat(query)\n",
        "            print(f\"Response: {response[:200]}...\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "# Run additional tests\n",
        "test_multilingual_capabilities(chatbot)\n",
        "test_edge_cases(chatbot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHmu_OMGpt4c",
        "outputId": "0672230c-7e94-4c90-e2f1-18a3f05792db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "MULTILINGUAL TESTING\n",
            "==================================================\n",
            "\n",
            "Query: What is offside in football?\n",
            "----------------------------------------\n",
            "Processing query: What is offside in football?\n",
            "Response: Je suis désolé, mais je ne peux répondre qu'aux questions liées au football. Pouvez-vous me poser une question sur le football s'il vous plaît?...\n",
            "==================================================\n",
            "\n",
            "Query: Qu'est-ce que le hors-jeu au football?\n",
            "----------------------------------------\n",
            "Processing query: Qu'est-ce que le hors-jeu au football?\n",
            "Response: Je suis désolé, mais je ne peux répondre qu'aux questions liées au football. Pouvez-vous me poser une question sur le football s'il vous plaît?...\n",
            "==================================================\n",
            "\n",
            "Query: Parlez-moi de l'équipe de France\n",
            "----------------------------------------\n",
            "Processing query: Parlez-moi de l'équipe de France\n",
            "Response: Je suis désolé, mais je ne peux répondre qu'aux questions liées au football. Pouvez-vous me poser une question sur le football s'il vous plaît?...\n",
            "==================================================\n",
            "\n",
            "Query: Tell me about the World Cup history\n",
            "----------------------------------------\n",
            "Processing query: Tell me about the World Cup history\n",
            "Response: Je suis désolé, mais je ne peux répondre qu'aux questions liées au football. Pouvez-vous me poser une question sur le football s'il vous plaît?...\n",
            "==================================================\n",
            "\n",
            "Query: Quels sont les plus grands joueurs français?\n",
            "----------------------------------------\n",
            "Processing query: Quels sont les plus grands joueurs français?\n",
            "Response: Je suis désolé, mais je ne peux répondre qu'aux questions liées au football. Pouvez-vous me poser une question sur le football s'il vous plaît?...\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "EDGE CASE TESTING\n",
            "==================================================\n",
            "\n",
            "Query: ''\n",
            "----------------------------------------\n",
            "Processing query: \n",
            "Response: Je suis désolé, mais je ne peux répondre qu'aux questions liées au football. Pouvez-vous me poser une question sur le football s'il vous plaît?...\n",
            "==================================================\n",
            "\n",
            "Query: 'Tell me about basketball'\n",
            "----------------------------------------\n",
            "Processing query: Tell me about basketball\n",
            "Response: Je suis désolé, mais je ne peux répondre qu'aux questions liées au football. Pouvez-vous me poser une question sur le football s'il vous plaît?...\n",
            "==================================================\n",
            "\n",
            "Query: 'What is 2+2?'\n",
            "----------------------------------------\n",
            "Processing query: What is 2+2?\n",
            "Response: Je suis désolé, mais je ne peux répondre qu'aux questions liées au football. Pouvez-vous me poser une question sur le football s'il vous plaît?...\n",
            "==================================================\n",
            "\n",
            "Query: 'Football'\n",
            "----------------------------------------\n",
            "Processing query: Football\n",
            "Response: Je suis désolé, mais je ne peux répondre qu'aux questions liées au football. Pouvez-vous me poser une question sur le football s'il vous plaît?...\n",
            "==================================================\n",
            "\n",
            "Query: 'Tell me everything about football in exactly 1000 words'\n",
            "----------------------------------------\n",
            "Processing query: Tell me everything about football in exactly 1000 words\n",
            "Response: Je suis désolé, mais je ne peux répondre qu'aux questions liées au football. Pouvez-vous me poser une question sur le football s'il vous plaît?...\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage Examples and Variations\n"
      ],
      "metadata": {
        "id": "Wz8RyyuNpzBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of how to modify the chatbot for different use cases\n",
        "\n",
        "# 1. Adjust retrieval parameters\n",
        "def create_specialized_chatbot(specialization=\"tactics\"):\n",
        "    \"\"\"Create a specialized version of the chatbot\"\"\"\n",
        "    specialized_chatbot = FootballChatbot(model, vectorstore)\n",
        "\n",
        "    if specialization == \"tactics\":\n",
        "        # Modify query enhancement for tactical focus\n",
        "        specialized_chatbot.query_enhancement_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=\"\"\"You are an expert at rephrasing football tactical queries.\n",
        "            Focus on formations, playing styles, coaching methods, and strategic aspects.\n",
        "            Generate 2-3 alternative phrasings that emphasize tactical analysis.\"\"\"),\n",
        "            HumanMessage(content=\"{question}\")\n",
        "        ])\n",
        "\n",
        "    return specialized_chatbot\n",
        "\n",
        "# 2. Create a version with different response styles\n",
        "def create_conversational_chatbot():\n",
        "    \"\"\"Create a more conversational version\"\"\"\n",
        "    conv_chatbot = FootballChatbot(model, vectorstore)\n",
        "\n",
        "    # Modify the answer generation prompt for conversational style\n",
        "    conv_chatbot.answer_generation_prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessage(content=\"\"\"You are a friendly football expert having a casual conversation.\n",
        "        Use a conversational tone, include interesting anecdotes, and make the response engaging.\n",
        "        Still maintain accuracy and cite sources, but in a more natural way.\"\"\"),\n",
        "        HumanMessage(content=\"Context: {context}\\n\\nQuestion: {question}\\n\\nPlease provide an engaging answer.\")\n",
        "    ])\n",
        "\n",
        "    return conv_chatbot\n",
        "\n",
        "# Example usage\n",
        "tactical_chatbot = create_specialized_chatbot(\"tactics\")\n",
        "conversational_chatbot = create_conversational_chatbot()"
      ],
      "metadata": {
        "id": "X7h50ynDp3pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Testing and Validation\n"
      ],
      "metadata": {
        "id": "P33bL11Mp8m7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprehensive testing function\n",
        "def comprehensive_test():\n",
        "    \"\"\"Run all tests to validate the chatbot functionality\"\"\"\n",
        "\n",
        "    print(\"Starting comprehensive chatbot testing...\")\n",
        "\n",
        "    # Test 1: Basic functionality\n",
        "    print(\"\\n1. Testing basic functionality...\")\n",
        "    basic_query = \"Tell me about the World Cup\"\n",
        "    response = chatbot.chat(basic_query)\n",
        "    print(f\"✓ Basic query processed: {len(response)} characters\")\n",
        "\n",
        "    # Test 2: Topic validation\n",
        "    print(\"\\n2. Testing topic validation...\")\n",
        "    football_query = \"Who won the Champions League?\"\n",
        "    non_football_query = \"What's the weather like?\"\n",
        "\n",
        "    is_football = chatbot.validate_topic(football_query)\n",
        "    is_not_football = chatbot.validate_topic(non_football_query)\n",
        "\n",
        "    print(f\"✓ Football query validation: {is_football}\")\n",
        "    print(f\"✓ Non-football query validation: {not is_not_football}\")\n",
        "\n",
        "    # Test 3: Multilingual support\n",
        "    print(\"\\n3. Testing multilingual support...\")\n",
        "    french_query = \"Parlez-moi de Zinedine Zidane\"\n",
        "    french_response = chatbot.chat(french_query)\n",
        "    print(f\"✓ French query processed: {len(french_response)} characters\")\n",
        "\n",
        "    # Test 4: Formatting instructions\n",
        "    print(\"\\n4. Testing formatting instructions...\")\n",
        "    formatted_query = \"Give me 3 bullet points about football tactics\"\n",
        "    formatted_response = chatbot.chat(formatted_query)\n",
        "    print(f\"✓ Formatted query processed: {len(formatted_response)} characters\")\n",
        "\n",
        "    print(\"\\n✅ All tests completed successfully!\")\n",
        "\n",
        "# Run comprehensive test\n",
        "comprehensive_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2637Y3NHqCBb",
        "outputId": "fd89b501-44ce-437d-b70a-28e38e904406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting comprehensive chatbot testing...\n",
            "\n",
            "1. Testing basic functionality...\n",
            "Processing query: Tell me about the World Cup\n",
            "✓ Basic query processed: 143 characters\n",
            "\n",
            "2. Testing topic validation...\n",
            "✓ Football query validation: False\n",
            "✓ Non-football query validation: True\n",
            "\n",
            "3. Testing multilingual support...\n",
            "Processing query: Parlez-moi de Zinedine Zidane\n",
            "✓ French query processed: 143 characters\n",
            "\n",
            "4. Testing formatting instructions...\n",
            "Processing query: Give me 3 bullet points about football tactics\n",
            "✓ Formatted query processed: 143 characters\n",
            "\n",
            "✅ All tests completed successfully!\n"
          ]
        }
      ]
    }
  ]
}