{"cells": [{"cell_type": "code", "metadata": {}, "source": ["# \ud83d\udccc Install required libraries\n", "!pip install spacy\n", "!python -m spacy download en_core_web_sm"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["# \ud83d\udccc Imports and Mount Google Drive\n", "import pandas as pd\n", "import numpy as np\n", "import os\n", "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.model_selection import GridSearchCV, train_test_split\n", "from sklearn.metrics import classification_report, accuracy_score\n", "import spacy\n", "from tqdm import tqdm\n", "\n", "# Load SpaCy model\n", "nlp = spacy.load(\"en_core_web_sm\")\n", "\n", "# Mount Google Drive to access CSVs\n", "from google.colab import drive\n", "drive.mount('/content/drive')"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["# \ud83d\udcc1 Load and concatenate all CSVs from Google Drive\n", "paths = [f\"/content/drive/My Drive/amazon-reviews-{i}.csv\" for i in range(1, 9)]\n", "dfs = [pd.read_csv(p) for p in paths]\n", "df = pd.concat(dfs, ignore_index=True)\n", "\n", "# \ud83d\udd0d Show column names and first few rows\n", "df.head()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\uddf9 Text Preprocessing: Lowercase and Lemmatization\n", "We apply SpaCy lemmatization and lowercase text to normalize vocabulary."]}, {"cell_type": "code", "metadata": {}, "source": ["# \ud83e\uddfc Lemmatization & Lowercase\n", "def preprocess_text(text):\n", "    doc = nlp(text.lower())\n", "    return ' '.join([token.lemma_ for token in doc if token.is_alpha])\n", "\n", "# Use only first 10,000 for faster vocabulary count\n", "sample_texts = df['reviewText'].dropna().sample(10000, random_state=42)\n", "lemmatized_texts = sample_texts.apply(preprocess_text)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udcca Vocabulary Size after Preprocessing\n", "Using `CountVectorizer` to count the unique vocabulary after lowercase + lemmatization."]}, {"cell_type": "code", "metadata": {}, "source": ["vectorizer = CountVectorizer()\n", "vectorizer.fit(lemmatized_texts)\n", "vocab_size = len(vectorizer.vocabulary_)\n", "print(f\"Vocabulary size after lowercase and lemmatization: {vocab_size}\")"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\udd16 Binary Classification: Predict if Review is 5-Star\n", "We build a pipeline with:\n", "- **TfidfVectorizer**\n", "- **LogisticRegression**\n", "\n", "And explore hyperparameters:\n", "- `C` for regularization\n", "- `token_pattern`\n", "- `use_idf`\n", "- `ngram_range`"]}, {"cell_type": "code", "metadata": {}, "source": ["# \ud83c\udfaf Restrict to 50,000 samples\n", "df = df[['reviewText', 'overall']].dropna()\n", "df = df.sample(50000, random_state=42)\n", "\n", "# \ud83d\udd04 Binary classification target\n", "df['label'] = (df['overall'] == 5).astype(int)\n", "X = df['reviewText']\n", "y = df['label']\n", "\n", "# \ud83d\udcc8 Split\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["# \ud83d\udccc Define pipeline\n", "pipeline = Pipeline([\n", "    ('tfidf', TfidfVectorizer()),\n", "    ('clf', LogisticRegression(max_iter=1000))\n", "])\n", "\n", "# \ud83e\uddea Grid search hyperparameters\n", "param_grid = {\n", "    'tfidf__min_df': [5],\n", "    'tfidf__max_df': [0.9],\n", "    'tfidf__token_pattern': [r'\\b[a-zA-Z]{3,}\\b'],\n", "    'tfidf__use_idf': [True, False],\n", "    'tfidf__ngram_range': [(1,1), (1,2)],\n", "    'clf__C': [0.01, 0.1, 1, 10],\n", "}\n", "\n", "# \ud83d\udd0d Grid search with 3-fold CV\n", "grid = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=2)\n", "grid.fit(X_train, y_train)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u2705 Best Model Results and Evaluation\n", "We now evaluate the best pipeline from the grid search on the test set."]}, {"cell_type": "code", "metadata": {}, "source": ["# \ud83d\udce2 Best parameters and accuracy\n", "print(\"Best Parameters:\", grid.best_params_)\n", "\n", "# \ud83e\uddea Test set performance\n", "y_pred = grid.predict(X_test)\n", "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n", "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"], "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.9"}}, "nbformat": 4, "nbformat_minor": 5}